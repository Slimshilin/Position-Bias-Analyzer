Reponse length
+----+-----------------------------+-----------+--------------------+
|    |            model            |   mean    |        std         |
+----+-----------------------------+-----------+--------------------+
| 0  |       vicuna-13b-v1.3       | 236.8625  | 179.92778021681366 |
| 1  |        baize-v2-13b         | 236.9125  | 197.14475606454766 |
| 2  |    palm-2-chat-bison-001    | 151.73125 | 168.76228554815646 |
| 3  |       nous-hermes-13b       | 144.80625 | 106.94405645447297 |
| 4  |       vicuna-7b-v1.3        | 232.66875 | 160.13657147396873 |
| 5  |      mpt-30b-instruct       |   117.2   | 122.58200316522813 |
| 6  |        wizardlm-30b         | 205.48125 |  151.799373017274  |
| 7  |         chatglm-6b          | 234.99375 | 146.4164222720167  |
| 8  |     falcon-40b-instruct     | 124.35625 | 139.21989561818202 |
| 9  |      rwkv-4-raven-14b       | 243.14375 | 248.61382822750932 |
| 10 |        mpt-30b-chat         |  206.425  | 173.3109398595484  |
| 11 |    oasst-sft-7-llama-30b    | 175.33125 | 161.61682005112434 |
| 12 |     gpt4all-13b-snoozy      |  178.925  | 163.40584253630593 |
| 13 |       vicuna-33b-v1.3       | 298.64375 | 194.11837068123538 |
| 14 |         alpaca-13b          | 73.21875  | 60.90624679322721  |
| 15 |         mpt-7b-chat         | 169.63125 | 133.1751488583268  |
| 16 |          llama-13b          |  160.275  | 258.99652386663416 |
| 17 | h2ogpt-oasst-open-llama-13b | 204.2125  | 202.11003771151496 |
| 18 |            gpt-4            | 252.9875  | 186.10380126088236 |
| 19 |          koala-13b          | 249.18125 | 222.4639822497959  |
| 20 |         guanaco-33b         |  241.775  | 170.72034698594072 |
| 21 |          claude-v1          |  225.15   | 133.7988322071609  |
| 22 |        gpt-3.5-turbo        | 201.75625 | 150.55828385026678 |
| 23 |          tulu-30b           | 157.8875  | 133.81021950415447 |
| 24 |        wizardlm-13b         | 197.7625  | 151.7606869177588  |
| 25 |   stablelm-tuned-alpha-7b   | 529.05625 | 385.7523533122481  |
| 26 |        dolly-v2-12b         | 108.65625 | 147.68535162952858 |
| 27 |      claude-instant-v1      | 210.56875 | 118.09607433542192 |
| 28 |         guanaco-65b         |  233.725  | 173.09148412039224 |
| 29 |   oasst-sft-4-pythia-12b    | 142.9625  | 161.4225002710279  |
| 30 |       fastchat-t5-3b        | 229.46875 | 188.7736052085606  |
+----+-----------------------------+-----------+--------------------+
Total comparisons: 4960. Failed to response: 6
Total comparisons: 4960. Non exact matches:  4800
Extracted 4718 answers from the responses (extraction success rate 98.29%)
3314 out of 4718 answers are consistent. (A vs. B <-> B vs. A) The consistent rate is 70.24%
(Win + Draw / 2) w. vicuna-13b-v1.3: 
+----+-----------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+--------+---------+
|    |            model            | Overall-lang |   EN   | coding | extraction | humanities |  math  | reasoning | roleplay |  stem  | writing |
+----+-----------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+--------+---------+
| 20 |       vicuna-13b-v1.3       |     nan      |  nan   |  nan   |    nan     |    nan     |  nan   |    nan    |   nan    |  nan   |   nan   |
| 6  |            gpt-4            |    0.8981    | 0.8981 | 0.875  |   0.6667   |    1.0     |  1.0   |    0.8    |  0.8182  |  1.0   | 0.9333  |
| 26 |          claude-v1          |    0.8362    | 0.8362 | 0.8333 |   0.6667   |   0.8571   | 0.8462 |  0.7778   |  0.7333  |  1.0   | 0.9231  |
| 4  |       vicuna-33b-v1.3       |    0.816     | 0.816  |  0.75  |   0.3333   |   0.8421   | 0.6667 |  0.8824   |   1.0    |  0.9   |   1.0   |
| 28 |      claude-instant-v1      |    0.7928    | 0.7928 | 0.875  |    0.8     |   0.8571   | 0.8571 |   0.75    |  0.9091  |  1.0   | 0.1667  |
| 23 |        wizardlm-30b         |    0.5625    | 0.5625 | 0.5333 |    1.0     |   0.5556   |  0.6   |  0.5714   |  0.6364  |  0.4   | 0.3333  |
| 3  |        gpt-3.5-turbo        |    0.5556    | 0.5556 | 0.6667 |   0.8333   |   0.2857   |  0.8   |    0.4    |  0.7143  | 0.4444 | 0.3333  |
| 5  |         guanaco-33b         |    0.5444    | 0.5444 | 0.4444 |    0.0     |    0.2     |  0.5   |  0.8333   |  0.8182  | 0.5556 |   0.5   |
| 2  |        wizardlm-13b         |    0.4731    | 0.4731 | 0.2727 |    0.4     |   0.3077   | 0.8571 |    0.5    |  0.6923  | 0.5263 | 0.3333  |
| 25 |        mpt-30b-chat         |    0.4545    | 0.4545 |  0.5   |   0.5455   |   0.5833   | 0.3333 |   0.25    |   0.75   | 0.4444 |   0.4   |
| 18 |         guanaco-65b         |    0.4444    | 0.4444 |  0.25  |    0.4     |   0.6667   |  0.2   |    0.5    |   0.5    |  0.6   |   0.5   |
| 21 |       vicuna-7b-v1.3        |    0.4231    | 0.4231 | 0.3333 |   0.5714   |    0.5     |  0.25  |  0.4286   |   0.25   |  0.5   |   0.5   |
| 0  |        baize-v2-13b         |    0.3868    | 0.3868 | 0.375  |    0.25    |    0.2     |  0.0   |  0.5556   |  0.8333  | 0.3158 | 0.4545  |
| 19 |          tulu-30b           |    0.3187    | 0.3187 | 0.1111 |    0.6     |   0.2727   | 0.6667 |  0.2857   |   0.25   |  0.25  |   0.2   |
| 11 |    oasst-sft-7-llama-30b    |    0.3084    | 0.3084 |  0.0   |    0.75    |   0.0909   | 0.3636 |  0.2222   |  0.5714  |  0.4   | 0.3333  |
| 22 |         mpt-7b-chat         |    0.2752    | 0.2752 | 0.1667 |    0.8     |   0.1053   | 0.1667 |  0.3333   |  0.2667  | 0.3158 |   0.2   |
| 29 |    palm-2-chat-bison-001    |    0.2617    | 0.2617 | 0.1333 |   0.3333   |   0.3333   |  0.2   |   0.25    |  0.2857  | 0.1429 | 0.4286  |
| 17 |         chatglm-6b          |     0.26     |  0.26  | 0.1765 |    0.5     |   0.1538   |  0.0   |  0.4167   |  0.6667  | 0.2353 | 0.1333  |
| 10 |          koala-13b          |    0.2476    | 0.2476 | 0.2222 |   0.3333   |   0.3333   |  0.25  |  0.3333   |  0.1667  | 0.1818 | 0.1429  |
| 9  |      rwkv-4-raven-14b       |    0.2315    | 0.2315 |  0.0   |    0.6     |    0.1     | 0.5455 |  0.1429   |  0.4667  | 0.1429 |   0.0   |
| 27 |     gpt4all-13b-snoozy      |    0.2264    | 0.2264 |  0.0   |   0.1667   |    0.5     |  0.0   |  0.2222   |  0.4286  | 0.2222 |  0.25   |
| 1  |      mpt-30b-instruct       |    0.2203    | 0.2203 | 0.3333 |   0.1818   |    0.0     |  0.5   |  0.2857   |  0.3529  |  0.0   |   0.2   |
| 12 | h2ogpt-oasst-open-llama-13b |    0.2034    | 0.2034 |  0.25  |    0.5     |   0.125    | 0.125  |  0.3333   |  0.125   |  0.0   | 0.1667  |
| 7  |   stablelm-tuned-alpha-7b   |    0.1556    | 0.1556 |  0.1   |   0.2941   |    0.0     |  0.0   |  0.2105   |   0.5    | 0.1111 |   0.0   |
| 8  |   oasst-sft-4-pythia-12b    |    0.1545    | 0.1545 | 0.1111 |    0.0     |    0.0     |  0.25  |  0.1053   |  0.625   |  0.2   |  0.125  |
| 30 |       nous-hermes-13b       |    0.1455    | 0.1455 | 0.125  |   0.1667   |   0.1429   |  0.25  |  0.3333   |  0.125   |  0.0   |   0.0   |
| 14 |     falcon-40b-instruct     |    0.1441    | 0.1441 | 0.1667 |    0.25    |    0.1     | 0.1429 |  0.1429   |  0.1111  | 0.1111 | 0.2143  |
| 16 |       fastchat-t5-3b        |    0.0787    | 0.0787 |  0.0   |   0.1333   |    0.0     |  0.0   |  0.2222   |  0.3333  |  0.0   |   0.0   |
| 24 |        dolly-v2-12b         |    0.0703    | 0.0703 | 0.1111 |   0.0769   |    0.0     |  0.0   |  0.1333   |  0.3077  |  0.0   |   0.0   |
| 15 |          llama-13b          |    0.0563    | 0.0563 |  0.0   |   0.125    |    0.0     | 0.1111 |  0.1333   |  0.1111  |  0.0   |   0.0   |
| 13 |         alpaca-13b          |    0.0476    | 0.0476 |  0.0   |    0.0     |    0.0     | 0.1429 |    0.1    |  0.1111  |  0.0   |   0.0   |
+----+-----------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+--------+---------+
(Win + Both Good) w. vicuna-13b-v1.3: 
+----+-----------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+--------+---------+
|    |            model            | Overall-lang |   EN   | coding | extraction | humanities |  math  | reasoning | roleplay |  stem  | writing |
+----+-----------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+--------+---------+
| 20 |       vicuna-13b-v1.3       |     nan      |  nan   |  nan   |    nan     |    nan     |  nan   |    nan    |   nan    |  nan   |   nan   |
| 6  |            gpt-4            |    0.8981    | 0.8981 | 0.875  |   0.6667   |    1.0     |  1.0   |    0.8    |  0.8182  |  1.0   | 0.9333  |
| 26 |          claude-v1          |    0.8362    | 0.8362 | 0.8333 |   0.6667   |   0.8571   | 0.8462 |  0.7778   |  0.7333  |  1.0   | 0.9231  |
| 4  |       vicuna-33b-v1.3       |    0.816     | 0.816  |  0.75  |   0.3333   |   0.8421   | 0.6667 |  0.8824   |   1.0    |  0.9   |   1.0   |
| 28 |      claude-instant-v1      |    0.7928    | 0.7928 | 0.875  |    0.8     |   0.8571   | 0.8571 |   0.75    |  0.9091  |  1.0   | 0.1667  |
| 23 |        wizardlm-30b         |    0.5625    | 0.5625 | 0.5333 |    1.0     |   0.5556   |  0.6   |  0.5714   |  0.6364  |  0.4   | 0.3333  |
| 3  |        gpt-3.5-turbo        |    0.5556    | 0.5556 | 0.6667 |   0.8333   |   0.2857   |  0.8   |    0.4    |  0.7143  | 0.4444 | 0.3333  |
| 5  |         guanaco-33b         |    0.5444    | 0.5444 | 0.4444 |    0.0     |    0.2     |  0.5   |  0.8333   |  0.8182  | 0.5556 |   0.5   |
| 2  |        wizardlm-13b         |    0.4731    | 0.4731 | 0.2727 |    0.4     |   0.3077   | 0.8571 |    0.5    |  0.6923  | 0.5263 | 0.3333  |
| 25 |        mpt-30b-chat         |    0.4545    | 0.4545 |  0.5   |   0.5455   |   0.5833   | 0.3333 |   0.25    |   0.75   | 0.4444 |   0.4   |
| 18 |         guanaco-65b         |    0.4444    | 0.4444 |  0.25  |    0.4     |   0.6667   |  0.2   |    0.5    |   0.5    |  0.6   |   0.5   |
| 21 |       vicuna-7b-v1.3        |    0.4359    | 0.4359 | 0.3333 |   0.7143   |    0.5     |  0.25  |  0.4286   |   0.25   |  0.5   |   0.5   |
| 0  |        baize-v2-13b         |    0.3868    | 0.3868 | 0.375  |    0.25    |    0.2     |  0.0   |  0.5556   |  0.8333  | 0.3158 | 0.4545  |
| 19 |          tulu-30b           |    0.3187    | 0.3187 | 0.1111 |    0.6     |   0.2727   | 0.6667 |  0.2857   |   0.25   |  0.25  |   0.2   |
| 11 |    oasst-sft-7-llama-30b    |    0.3084    | 0.3084 |  0.0   |    0.75    |   0.0909   | 0.3636 |  0.2222   |  0.5714  |  0.4   | 0.3333  |
| 22 |         mpt-7b-chat         |    0.2752    | 0.2752 | 0.1667 |    0.8     |   0.1053   | 0.1667 |  0.3333   |  0.2667  | 0.3158 |   0.2   |
| 17 |         chatglm-6b          |     0.27     |  0.27  | 0.2353 |    0.5     |   0.1538   |  0.0   |  0.4167   |  0.6667  | 0.2353 | 0.1333  |
| 29 |    palm-2-chat-bison-001    |    0.2617    | 0.2617 | 0.1333 |   0.3333   |   0.3333   |  0.2   |   0.25    |  0.2857  | 0.1429 | 0.4286  |
| 10 |          koala-13b          |    0.2476    | 0.2476 | 0.2222 |   0.3333   |   0.3333   |  0.25  |  0.3333   |  0.1667  | 0.1818 | 0.1429  |
| 9  |      rwkv-4-raven-14b       |    0.2315    | 0.2315 |  0.0   |    0.6     |    0.1     | 0.5455 |  0.1429   |  0.4667  | 0.1429 |   0.0   |
| 27 |     gpt4all-13b-snoozy      |    0.2264    | 0.2264 |  0.0   |   0.1667   |    0.5     |  0.0   |  0.2222   |  0.4286  | 0.2222 |  0.25   |
| 1  |      mpt-30b-instruct       |    0.2203    | 0.2203 | 0.3333 |   0.1818   |    0.0     |  0.5   |  0.2857   |  0.3529  |  0.0   |   0.2   |
| 12 | h2ogpt-oasst-open-llama-13b |    0.2034    | 0.2034 |  0.25  |    0.5     |   0.125    | 0.125  |  0.3333   |  0.125   |  0.0   | 0.1667  |
| 7  |   stablelm-tuned-alpha-7b   |    0.1556    | 0.1556 |  0.1   |   0.2941   |    0.0     |  0.0   |  0.2105   |   0.5    | 0.1111 |   0.0   |
| 8  |   oasst-sft-4-pythia-12b    |    0.1545    | 0.1545 | 0.1111 |    0.0     |    0.0     |  0.25  |  0.1053   |  0.625   |  0.2   |  0.125  |
| 30 |       nous-hermes-13b       |    0.1455    | 0.1455 | 0.125  |   0.1667   |   0.1429   |  0.25  |  0.3333   |  0.125   |  0.0   |   0.0   |
| 14 |     falcon-40b-instruct     |    0.1441    | 0.1441 | 0.1667 |    0.25    |    0.1     | 0.1429 |  0.1429   |  0.1111  | 0.1111 | 0.2143  |
| 16 |       fastchat-t5-3b        |    0.0787    | 0.0787 |  0.0   |   0.1333   |    0.0     |  0.0   |  0.2222   |  0.3333  |  0.0   |   0.0   |
| 24 |        dolly-v2-12b         |    0.0703    | 0.0703 | 0.1111 |   0.0769   |    0.0     |  0.0   |  0.1333   |  0.3077  |  0.0   |   0.0   |
| 15 |          llama-13b          |    0.0563    | 0.0563 |  0.0   |   0.125    |    0.0     | 0.1111 |  0.1333   |  0.1111  |  0.0   |   0.0   |
| 13 |         alpaca-13b          |    0.0476    | 0.0476 |  0.0   |    0.0     |    0.0     | 0.1429 |    0.1    |  0.1111  |  0.0   |   0.0   |
+----+-----------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+--------+---------+
