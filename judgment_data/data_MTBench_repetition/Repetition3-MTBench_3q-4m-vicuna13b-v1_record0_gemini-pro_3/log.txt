Reponse length
+---+-------------------------+--------------------+--------------------+
|   |          model          |        mean        |        std         |
+---+-------------------------+--------------------+--------------------+
| 0 |     vicuna-13b-v1.3     | 229.95833333333334 | 170.34364266746857 |
| 1 | stablelm-tuned-alpha-7b | 466.9583333333333  | 378.1559399646601  |
| 2 |   falcon-40b-instruct   | 126.47916666666667 | 132.60451688877552 |
| 3 |        claude-v1        | 209.58333333333334 | 126.4339144463313  |
| 4 |        llama-13b        |      151.0625      | 269.6300247754628  |
+---+-------------------------+--------------------+--------------------+
Total comparisons: 192. Failed to response: 0
Total comparisons: 192. Non exact matches:  192
Extracted 192 answers from the responses (extraction success rate 100.00%)
136 out of 192 answers are consistent. (A vs. B <-> B vs. A) The consistent rate is 70.83%
(Win + Draw / 2) w. vicuna-13b-v1.3: 
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
|   |          model          | Overall-lang |   EN   | coding | extraction | humanities | math | reasoning | roleplay | stem | writing |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
| 3 |     vicuna-13b-v1.3     |     nan      |  nan   |  nan   |    nan     |    nan     | nan  |    nan    |   nan    | nan  |   nan   |
| 1 |        claude-v1        |    0.8125    | 0.8125 | 0.6667 |    0.5     |    1.0     | 1.0  |    0.0    |   1.0    | 1.0  |   1.0   |
| 2 | stablelm-tuned-alpha-7b |    0.0667    | 0.0667 |  0.0   |    0.0     |    0.0     | 0.0  |    0.0    |   0.5    | 0.0  |   0.0   |
| 0 |   falcon-40b-instruct   |    0.0556    | 0.0556 |  0.0   |    0.0     |    0.0     | 0.0  |    0.0    |  0.3333  | 0.0  |   0.0   |
| 4 |        llama-13b        |     0.0      |  0.0   |  0.0   |    0.0     |    0.0     | 0.0  |    0.0    |   0.0    | 0.0  |   0.0   |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
(Win + Both Good) w. vicuna-13b-v1.3: 
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
|   |          model          | Overall-lang |   EN   | coding | extraction | humanities | math | reasoning | roleplay | stem | writing |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
| 3 |     vicuna-13b-v1.3     |     nan      |  nan   |  nan   |    nan     |    nan     | nan  |    nan    |   nan    | nan  |   nan   |
| 1 |        claude-v1        |    0.8125    | 0.8125 | 0.6667 |    0.5     |    1.0     | 1.0  |    0.0    |   1.0    | 1.0  |   1.0   |
| 2 | stablelm-tuned-alpha-7b |    0.0667    | 0.0667 |  0.0   |    0.0     |    0.0     | 0.0  |    0.0    |   0.5    | 0.0  |   0.0   |
| 0 |   falcon-40b-instruct   |    0.0556    | 0.0556 |  0.0   |    0.0     |    0.0     | 0.0  |    0.0    |  0.3333  | 0.0  |   0.0   |
| 4 |        llama-13b        |     0.0      |  0.0   |  0.0   |    0.0     |    0.0     | 0.0  |    0.0    |   0.0    | 0.0  |   0.0   |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
