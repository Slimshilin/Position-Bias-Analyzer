Reponse length
+---+-------------------------+--------------------+--------------------+
|   |          model          |        mean        |        std         |
+---+-------------------------+--------------------+--------------------+
| 0 |     vicuna-13b-v1.3     | 229.95833333333334 | 170.34364266746857 |
| 1 | stablelm-tuned-alpha-7b | 466.9583333333333  | 378.1559399646601  |
| 2 |   falcon-40b-instruct   | 126.47916666666667 | 132.60451688877552 |
| 3 |        claude-v1        | 209.58333333333334 | 126.4339144463313  |
| 4 |        llama-13b        |      151.0625      | 269.6300247754628  |
+---+-------------------------+--------------------+--------------------+
Total comparisons: 192. Failed to response: 0
Total comparisons: 192. Non exact matches:  192
Extracted 192 answers from the responses (extraction success rate 100.00%)
154 out of 192 answers are consistent. (A vs. B <-> B vs. A) The consistent rate is 80.21%
(Win + Draw / 2) w. vicuna-13b-v1.3: 
+---+-------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+------+---------+
|   |          model          | Overall-lang |   EN   | coding | extraction | humanities |  math  | reasoning | roleplay | stem | writing |
+---+-------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+------+---------+
| 0 |     vicuna-13b-v1.3     |     nan      |  nan   |  nan   |    nan     |    nan     |  nan   |    nan    |   nan    | nan  |   nan   |
| 2 |        claude-v1        |    0.9211    | 0.9211 |  0.75  |    1.0     |   0.8333   | 0.8333 |    1.0    |   1.0    | 1.0  |   1.0   |
| 1 |   falcon-40b-instruct   |    0.3529    | 0.3529 |  0.75  |    0.75    |   0.3333   |  0.5   |    0.0    |  0.3333  | 0.0  |   0.0   |
| 3 | stablelm-tuned-alpha-7b |    0.125     | 0.125  |  0.25  |    0.0     |    0.0     | 0.6667 |    0.0    |   0.0    | 0.0  |   0.0   |
| 4 |        llama-13b        |    0.0952    | 0.0952 |  0.0   |    0.0     |    0.0     |  0.5   |    0.0    |  0.3333  | 0.0  |   0.0   |
+---+-------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+------+---------+
(Win + Both Good) w. vicuna-13b-v1.3: 
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
|   |          model          | Overall-lang |   EN   | coding | extraction | humanities | math | reasoning | roleplay | stem | writing |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
| 0 |     vicuna-13b-v1.3     |     nan      |  nan   |  nan   |    nan     |    nan     | nan  |    nan    |   nan    | nan  |   nan   |
| 2 |        claude-v1        |     1.0      |  1.0   |  1.0   |    1.0     |    1.0     | 1.0  |    1.0    |   1.0    | 1.0  |   1.0   |
| 1 |   falcon-40b-instruct   |    0.4706    | 0.4706 |  1.0   |    1.0     |   0.3333   | 1.0  |    0.0    |  0.3333  | 0.0  |   0.0   |
| 3 | stablelm-tuned-alpha-7b |     0.2      |  0.2   |  0.5   |    0.0     |    0.0     | 1.0  |    0.0    |   0.0    | 0.0  |   0.0   |
| 4 |        llama-13b        |    0.1429    | 0.1429 |  0.0   |    0.0     |    0.0     | 1.0  |    0.0    |  0.3333  | 0.0  |   0.0   |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
