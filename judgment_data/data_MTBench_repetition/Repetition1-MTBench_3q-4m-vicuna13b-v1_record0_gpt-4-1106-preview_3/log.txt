Reponse length
+---+-------------------------+--------------------+--------------------+
|   |          model          |        mean        |        std         |
+---+-------------------------+--------------------+--------------------+
| 0 |     vicuna-13b-v1.3     | 229.95833333333334 | 170.34364266746857 |
| 1 | stablelm-tuned-alpha-7b | 466.9583333333333  | 378.1559399646601  |
| 2 |   falcon-40b-instruct   | 126.47916666666667 | 132.60451688877552 |
| 3 |        claude-v1        | 209.58333333333334 | 126.4339144463313  |
| 4 |        llama-13b        |      151.0625      | 269.6300247754628  |
+---+-------------------------+--------------------+--------------------+
Total comparisons: 192. Failed to response: 0
Total comparisons: 192. Non exact matches:  192
Extracted 192 answers from the responses (extraction success rate 100.00%)
156 out of 192 answers are consistent. (A vs. B <-> B vs. A) The consistent rate is 81.25%
(Win + Draw / 2) w. vicuna-13b-v1.3: 
+---+-------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+------+---------+
|   |          model          | Overall-lang |   EN   | coding | extraction | humanities |  math  | reasoning | roleplay | stem | writing |
+---+-------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+------+---------+
| 2 |     vicuna-13b-v1.3     |     nan      |  nan   |  nan   |    nan     |    nan     |  nan   |    nan    |   nan    | nan  |   nan   |
| 0 |        claude-v1        |    0.8947    | 0.8947 |  0.75  |    1.0     |    1.0     | 0.8333 |    0.5    |   1.0    | 1.0  |   1.0   |
| 4 |   falcon-40b-instruct   |    0.2941    | 0.2941 | 0.6667 |    0.0     |   0.3333   |  0.5   |    0.0    |  0.3333  | 0.0  |   0.0   |
| 1 | stablelm-tuned-alpha-7b |    0.1591    | 0.1591 |  0.25  |    0.0     |    0.0     | 0.6667 |    0.0    |  0.3333  | 0.0  |   0.0   |
| 3 |        llama-13b        |     0.1      |  0.1   | 0.3333 |    0.0     |    0.0     |  0.5   |    0.0    |   0.25   | 0.0  |   0.0   |
+---+-------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+------+---------+
(Win + Both Good) w. vicuna-13b-v1.3: 
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
|   |          model          | Overall-lang |   EN   | coding | extraction | humanities | math | reasoning | roleplay | stem | writing |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
| 2 |     vicuna-13b-v1.3     |     nan      |  nan   |  nan   |    nan     |    nan     | nan  |    nan    |   nan    | nan  |   nan   |
| 0 |        claude-v1        |    0.9474    | 0.9474 |  1.0   |    1.0     |    1.0     | 1.0  |    0.5    |   1.0    | 1.0  |   1.0   |
| 4 |   falcon-40b-instruct   |    0.4118    | 0.4118 |  1.0   |    0.0     |   0.3333   | 1.0  |    0.0    |  0.3333  | 0.0  |   0.0   |
| 1 | stablelm-tuned-alpha-7b |    0.2273    | 0.2273 |  0.5   |    0.0     |    0.0     | 1.0  |    0.0    |  0.3333  | 0.0  |   0.0   |
| 3 |        llama-13b        |     0.2      |  0.2   | 0.6667 |    0.0     |    0.0     | 1.0  |    0.0    |   0.5    | 0.0  |   0.0   |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
