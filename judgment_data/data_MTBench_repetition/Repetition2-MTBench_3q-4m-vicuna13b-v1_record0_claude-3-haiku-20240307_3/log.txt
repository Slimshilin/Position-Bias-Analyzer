Reponse length
+---+-------------------------+--------------------+--------------------+
|   |          model          |        mean        |        std         |
+---+-------------------------+--------------------+--------------------+
| 0 |     vicuna-13b-v1.3     | 229.95833333333334 | 170.34364266746857 |
| 1 | stablelm-tuned-alpha-7b | 466.9583333333333  | 378.1559399646601  |
| 2 |   falcon-40b-instruct   | 126.47916666666667 | 132.60451688877552 |
| 3 |        claude-v1        | 209.58333333333334 | 126.4339144463313  |
| 4 |        llama-13b        |      151.0625      | 269.6300247754628  |
+---+-------------------------+--------------------+--------------------+
Total comparisons: 192. Failed to response: 0
Total comparisons: 192. Non exact matches:  192
Extracted 191 answers from the responses (extraction success rate 99.48%)
129 out of 191 answers are consistent. (A vs. B <-> B vs. A) The consistent rate is 67.54%
(Win + Draw / 2) w. vicuna-13b-v1.3: 
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
|   |          model          | Overall-lang |   EN   | coding | extraction | humanities | math | reasoning | roleplay | stem | writing |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
| 2 |     vicuna-13b-v1.3     |     nan      |  nan   |  nan   |    nan     |    nan     | nan  |    nan    |   nan    | nan  |   nan   |
| 0 |        claude-v1        |    0.6364    | 0.6364 |  0.0   |   0.3333   |    0.5     | 1.0  |    0.0    |   1.0    | 1.0  |   1.0   |
| 4 | stablelm-tuned-alpha-7b |    0.125     | 0.125  |  0.0   |    0.5     |    0.0     | 0.0  |    0.0    |   0.5    | 0.0  |   0.0   |
| 3 |        llama-13b        |    0.0444    | 0.0444 |  0.0   |    0.4     |    0.0     | 0.0  |    0.0    |   0.0    | 0.0  |   0.0   |
| 1 |   falcon-40b-instruct   |    0.0333    | 0.0333 |  0.0   |    0.0     |    0.0     | 0.25 |    0.0    |   0.0    | 0.0  |   0.0   |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
(Win + Both Good) w. vicuna-13b-v1.3: 
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
|   |          model          | Overall-lang |   EN   | coding | extraction | humanities | math | reasoning | roleplay | stem | writing |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
| 2 |     vicuna-13b-v1.3     |     nan      |  nan   |  nan   |    nan     |    nan     | nan  |    nan    |   nan    | nan  |   nan   |
| 0 |        claude-v1        |    0.6364    | 0.6364 |  0.0   |   0.3333   |    0.5     | 1.0  |    0.0    |   1.0    | 1.0  |   1.0   |
| 4 | stablelm-tuned-alpha-7b |    0.125     | 0.125  |  0.0   |    0.5     |    0.0     | 0.0  |    0.0    |   0.5    | 0.0  |   0.0   |
| 1 |   falcon-40b-instruct   |    0.0667    | 0.0667 |  0.0   |    0.0     |    0.0     | 0.5  |    0.0    |   0.0    | 0.0  |   0.0   |
| 3 |        llama-13b        |    0.0444    | 0.0444 |  0.0   |    0.4     |    0.0     | 0.0  |    0.0    |   0.0    | 0.0  |   0.0   |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+------+---------+
