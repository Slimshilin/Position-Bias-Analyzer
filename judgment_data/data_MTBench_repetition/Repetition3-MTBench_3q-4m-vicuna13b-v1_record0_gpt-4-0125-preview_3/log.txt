Reponse length
+---+-------------------------+--------------------+--------------------+
|   |          model          |        mean        |        std         |
+---+-------------------------+--------------------+--------------------+
| 0 |     vicuna-13b-v1.3     | 229.95833333333334 | 170.34364266746857 |
| 1 | stablelm-tuned-alpha-7b | 466.9583333333333  | 378.1559399646601  |
| 2 |   falcon-40b-instruct   | 126.47916666666667 | 132.60451688877552 |
| 3 |        claude-v1        | 209.58333333333334 | 126.4339144463313  |
| 4 |        llama-13b        |      151.0625      | 269.6300247754628  |
+---+-------------------------+--------------------+--------------------+
Total comparisons: 192. Failed to response: 0
Total comparisons: 192. Non exact matches:  192
Extracted 192 answers from the responses (extraction success rate 100.00%)
164 out of 192 answers are consistent. (A vs. B <-> B vs. A) The consistent rate is 85.42%
(Win + Draw / 2) w. vicuna-13b-v1.3: 
+---+-------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+--------+---------+
|   |          model          | Overall-lang |   EN   | coding | extraction | humanities |  math  | reasoning | roleplay |  stem  | writing |
+---+-------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+--------+---------+
| 3 |     vicuna-13b-v1.3     |     nan      |  nan   |  nan   |    nan     |    nan     |  nan   |    nan    |   nan    |  nan   |   nan   |
| 4 |        claude-v1        |     0.85     |  0.85  |  0.75  |   0.8333   |   0.8333   | 0.8333 |    0.5    |   1.0    |  1.0   |   1.0   |
| 1 |   falcon-40b-instruct   |    0.2619    | 0.2619 |  0.5   |    0.0     |   0.3333   |  0.5   |   0.25    |  0.3333  | 0.1667 |   0.0   |
| 0 |        llama-13b        |    0.1136    | 0.1136 | 0.1667 |    0.0     |    0.0     |  0.5   |    0.0    |  0.3333  |  0.0   |   0.0   |
| 2 | stablelm-tuned-alpha-7b |    0.0789    | 0.0789 |  0.0   |    0.0     |    0.0     |  0.75  |    0.0    |   0.0    |  0.0   |   0.0   |
+---+-------------------------+--------------+--------+--------+------------+------------+--------+-----------+----------+--------+---------+
(Win + Both Good) w. vicuna-13b-v1.3: 
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+--------+---------+
|   |          model          | Overall-lang |   EN   | coding | extraction | humanities | math | reasoning | roleplay |  stem  | writing |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+--------+---------+
| 3 |     vicuna-13b-v1.3     |     nan      |  nan   |  nan   |    nan     |    nan     | nan  |    nan    |   nan    |  nan   |   nan   |
| 4 |        claude-v1        |     0.95     |  0.95  |  1.0   |    1.0     |    1.0     | 1.0  |    0.5    |   1.0    |  1.0   |   1.0   |
| 1 |   falcon-40b-instruct   |    0.381     | 0.381  |  0.5   |    0.0     |   0.3333   | 1.0  |    0.5    |  0.3333  | 0.3333 |   0.0   |
| 0 |        llama-13b        |    0.1818    | 0.1818 | 0.3333 |    0.0     |    0.0     | 1.0  |    0.0    |  0.3333  |  0.0   |   0.0   |
| 2 | stablelm-tuned-alpha-7b |    0.1053    | 0.1053 |  0.0   |    0.0     |    0.0     | 1.0  |    0.0    |   0.0    |  0.0   |   0.0   |
+---+-------------------------+--------------+--------+--------+------------+------------+------+-----------+----------+--------+---------+
